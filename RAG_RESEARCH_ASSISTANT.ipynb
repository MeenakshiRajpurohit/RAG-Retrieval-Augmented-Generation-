{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMSJMZngXhXqOhUqNzB5GEz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "56ba4215c9b94192b50fc1e40fa10ef3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_037fb92040f44c0791257f1e3f288c30",
              "IPY_MODEL_50ec1d326e704433bac9a59e0f487f13",
              "IPY_MODEL_b1a7c26048d74e9db9b1c44f1f713f8a"
            ],
            "layout": "IPY_MODEL_f8810c42f1134da0a3efefe47df2e7a1"
          }
        },
        "037fb92040f44c0791257f1e3f288c30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ef1aa37e84a4e35b964a944884b0013",
            "placeholder": "​",
            "style": "IPY_MODEL_90892646c03240f7a3c7e971fbeac837",
            "value": "Loading weights: 100%"
          }
        },
        "50ec1d326e704433bac9a59e0f487f13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c92fca2cf6640adb476672a19e60c7c",
            "max": 103,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_747c0da8ef3b43b49e0241d0865e50aa",
            "value": 103
          }
        },
        "b1a7c26048d74e9db9b1c44f1f713f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_693969e80fd94535932ca014255a6193",
            "placeholder": "​",
            "style": "IPY_MODEL_9165150f9bd04246a6594f60ba09d38d",
            "value": " 103/103 [00:00&lt;00:00, 1035.89it/s, Materializing param=pooler.dense.weight]"
          }
        },
        "f8810c42f1134da0a3efefe47df2e7a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ef1aa37e84a4e35b964a944884b0013": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90892646c03240f7a3c7e971fbeac837": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c92fca2cf6640adb476672a19e60c7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "747c0da8ef3b43b49e0241d0865e50aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "693969e80fd94535932ca014255a6193": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9165150f9bd04246a6594f60ba09d38d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bbb9433795e4db3b214309f2f1eab07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c2617087a95d40b7bb953688142e4936",
              "IPY_MODEL_570ac49f254e4eee91b56c2a117d6eb0",
              "IPY_MODEL_c518a599a3f84e0f90eb958a851a4c90"
            ],
            "layout": "IPY_MODEL_02e4d3fe8eb34db683b0be6e8c76beed"
          }
        },
        "c2617087a95d40b7bb953688142e4936": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d1727ae68ef49a0a2d12844c745ac2e",
            "placeholder": "​",
            "style": "IPY_MODEL_c01cb71c13e548eb94521576f2cb1556",
            "value": "Loading weights: 100%"
          }
        },
        "570ac49f254e4eee91b56c2a117d6eb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f2d37c579534681b3fbfa8ea6b31638",
            "max": 282,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f914e6701fef420e849a14e8afdb89db",
            "value": 282
          }
        },
        "c518a599a3f84e0f90eb958a851a4c90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cd52b69cbb449d5b2c1bc3f8a90ba56",
            "placeholder": "​",
            "style": "IPY_MODEL_01d020eeda2548c3b1309322b3e97ccf",
            "value": " 282/282 [00:00&lt;00:00, 1295.28it/s, Materializing param=shared.weight]"
          }
        },
        "02e4d3fe8eb34db683b0be6e8c76beed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d1727ae68ef49a0a2d12844c745ac2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c01cb71c13e548eb94521576f2cb1556": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f2d37c579534681b3fbfa8ea6b31638": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f914e6701fef420e849a14e8afdb89db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5cd52b69cbb449d5b2c1bc3f8a90ba56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01d020eeda2548c3b1309322b3e97ccf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7033d328456c4be6a201d52c4870f4c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7b36604aafa648f59ddc4891d08dab0b",
              "IPY_MODEL_486dbfc600eb43199eaa08cf992f4bb4",
              "IPY_MODEL_85fdd57f64154e7894cdc4e93d25c17d"
            ],
            "layout": "IPY_MODEL_e0444194af25486db071313d2f71c85b"
          }
        },
        "7b36604aafa648f59ddc4891d08dab0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d31e6c9be06640a49f22882abd8d81b0",
            "placeholder": "​",
            "style": "IPY_MODEL_425a84dd1b704fa891dd252b6b0c31df",
            "value": "Loading weights: 100%"
          }
        },
        "486dbfc600eb43199eaa08cf992f4bb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98e65565a3e2497499adeb253bdd6e90",
            "max": 103,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7ba0ffa369264038a6a1bcecaf8f9d46",
            "value": 103
          }
        },
        "85fdd57f64154e7894cdc4e93d25c17d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8324313d6bbc4be1996bf4ef10b2b841",
            "placeholder": "​",
            "style": "IPY_MODEL_983ca30c4c724bc7a6edff62961e9377",
            "value": " 103/103 [00:00&lt;00:00, 1042.89it/s, Materializing param=pooler.dense.weight]"
          }
        },
        "e0444194af25486db071313d2f71c85b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d31e6c9be06640a49f22882abd8d81b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "425a84dd1b704fa891dd252b6b0c31df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98e65565a3e2497499adeb253bdd6e90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ba0ffa369264038a6a1bcecaf8f9d46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8324313d6bbc4be1996bf4ef10b2b841": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "983ca30c4c724bc7a6edff62961e9377": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1289d310a0c74dacae8664ba5c898054": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a4c48ae10514810a0201c8003fd0045",
              "IPY_MODEL_0ca00c6bfb5444c0af909bcd1984c0ae",
              "IPY_MODEL_265823666e5f492d8f64e9f27f1f4c54"
            ],
            "layout": "IPY_MODEL_d56402835ec24b7e954160b115c419b9"
          }
        },
        "9a4c48ae10514810a0201c8003fd0045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d8cdc488a2044ddb8990a79f9c7da8b",
            "placeholder": "​",
            "style": "IPY_MODEL_3def824c40674786b98bb334bc5df144",
            "value": "Loading weights: 100%"
          }
        },
        "0ca00c6bfb5444c0af909bcd1984c0ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bc5d76625ea4a6797e96569f334c4b0",
            "max": 282,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d4e507262d248559697c99150025ba0",
            "value": 282
          }
        },
        "265823666e5f492d8f64e9f27f1f4c54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84d52f372e34401fa8018d4e444efab8",
            "placeholder": "​",
            "style": "IPY_MODEL_c0a659d3ea7747b0b305fcfed120cd59",
            "value": " 282/282 [00:00&lt;00:00, 1306.83it/s, Materializing param=shared.weight]"
          }
        },
        "d56402835ec24b7e954160b115c419b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d8cdc488a2044ddb8990a79f9c7da8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3def824c40674786b98bb334bc5df144": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1bc5d76625ea4a6797e96569f334c4b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d4e507262d248559697c99150025ba0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "84d52f372e34401fa8018d4e444efab8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0a659d3ea7747b0b305fcfed120cd59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MeenakshiRajpurohit/RAG-Retrieval-Augmented-Generation-/blob/main/RAG_RESEARCH_ASSISTANT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# RAG RESEARCH ASSISTANT - FINAL FIXED VERSION\n",
        "# Google Colab Compatible - All errors fixed\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 1: INSTALL DEPENDENCIES\n",
        "# ============================================================================\n",
        "\n",
        "!pip install -q langchain langchain-community langchain-core\n",
        "!pip install -q faiss-cpu sentence-transformers\n",
        "!pip install -q torch transformers huggingface-hub\n",
        "!pip install -q tqdm\n",
        "\n",
        "print(\"✓ All packages installed!\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 2: IMPORTS\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"✓ All imports successful!\")\n",
        "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 3: PAPER DATABASE\n",
        "# ============================================================================\n",
        "\n",
        "PAPERS_DATABASE = {\n",
        "    \"NLP\": [\n",
        "        {\n",
        "            \"title\": \"Attention Is All You Need\",\n",
        "            \"authors\": [\"Vaswani\", \"Shazeer\", \"Parmar\"],\n",
        "            \"published\": \"2017-06-12\",\n",
        "            \"arxiv_id\": \"1706.03762\",\n",
        "            \"category\": \"NLP\",\n",
        "            \"summary\": \"\"\"Transformers have become ubiquitous in NLP. The architecture uses self-attention mechanisms instead of recurrence.\n",
        "            Key features: (1) Processes sequences in parallel for fast training, (2) Multi-head attention allows focusing on different parts,\n",
        "            (3) Positional encodings capture sequence order, (4) Encoder-decoder architecture enables many applications.\n",
        "            Impact: Foundation for BERT, GPT, T5, and modern language models.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\n",
        "            \"authors\": [\"Devlin\", \"Chang\", \"Lee\", \"Toutanova\"],\n",
        "            \"published\": \"2018-10-11\",\n",
        "            \"arxiv_id\": \"1810.04805\",\n",
        "            \"category\": \"NLP\",\n",
        "            \"summary\": \"\"\"BERT introduces bidirectional pre-training for language models. Key innovation: Masked Language Modeling (MLM)\n",
        "            where random words are masked during training. The model must predict masked tokens using context from both left and right.\n",
        "            Improvements: (1) Bidirectional context, (2) Masked token prediction, (3) Next sentence prediction task.\n",
        "            Impact: Significantly improved GLUE benchmark scores and showed pre-training benefits.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Language Models are Unsupervised Multitask Learners\",\n",
        "            \"authors\": [\"Radford\", \"Wu\", \"Child\", \"Luan\"],\n",
        "            \"published\": \"2019-02-14\",\n",
        "            \"arxiv_id\": \"1902.10165\",\n",
        "            \"category\": \"NLP\",\n",
        "            \"summary\": \"\"\"GPT-2 demonstrates that large language models trained on diverse text learn multiple tasks without explicit supervision.\n",
        "            Key findings: (1) Scaling improves performance, (2) Models learn tasks naturally from data, (3) No task-specific training needed.\n",
        "            Applications: Machine translation, summarization, question-answering all work with same base model.\n",
        "            Impact: Showed the power of scale and diversity in language modeling.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"RoFormer: Enhanced Transformer with Rotary Position Embedding\",\n",
        "            \"authors\": [\"Su\", \"Ahmed\", \"Lu\", \"Pan\"],\n",
        "            \"published\": \"2021-04-20\",\n",
        "            \"arxiv_id\": \"2104.09864\",\n",
        "            \"category\": \"NLP\",\n",
        "            \"summary\": \"\"\"Position encoding is crucial for capturing sequence order. RoFormer proposes Rotary Position Embeddings (RoPE).\n",
        "            Key idea: Encode position using rotation matrices instead of additive encodings.\n",
        "            Advantages: (1) Better extrapolation to longer sequences, (2) Natural representation of relative positions, (3) Improved generalization.\n",
        "            Impact: Used in LLaMA and other modern language models.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"LLaMA: Open and Efficient Foundation Language Models\",\n",
        "            \"authors\": [\"Touvron\", \"Lavril\", \"Izacard\"],\n",
        "            \"published\": \"2023-02-27\",\n",
        "            \"arxiv_id\": \"2302.13971\",\n",
        "            \"category\": \"NLP\",\n",
        "            \"summary\": \"\"\"LLaMA shows that efficient training on public data can match proprietary models.\n",
        "            Key achievements: (1) 7B-65B parameter models, (2) Trained on 1.4T public tokens, (3) LLaMA-13B outperforms GPT-3 65B.\n",
        "            Techniques: (1) Careful data curation, (2) Efficient scaling, (3) Improved training recipes.\n",
        "            Impact: Democratized large language models by proving public data suffices.\"\"\"\n",
        "        }\n",
        "    ],\n",
        "    \"Computer Vision\": [\n",
        "        {\n",
        "            \"title\": \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\",\n",
        "            \"authors\": [\"Dosovitskiy\", \"Beyer\", \"Kolesnikov\"],\n",
        "            \"published\": \"2020-10-22\",\n",
        "            \"arxiv_id\": \"2010.11929\",\n",
        "            \"category\": \"Computer Vision\",\n",
        "            \"summary\": \"\"\"Vision Transformer (ViT) applies pure transformer architecture to image classification.\n",
        "            Key idea: Divide image into patches (16x16), treat patches as tokens like in NLP.\n",
        "            Architecture: (1) Linear projection of patches, (2) Positional encodings, (3) Standard transformer encoder.\n",
        "            Results: Competitive with CNNs when trained on large datasets, better scaling properties.\n",
        "            Impact: Showed transformers work beyond NLP, inspired multimodal models.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Masked Autoencoders Are Scalable Vision Learners\",\n",
        "            \"authors\": [\"He\", \"Chen\", \"Xie\", \"Li\"],\n",
        "            \"published\": \"2021-11-11\",\n",
        "            \"arxiv_id\": \"2111.06377\",\n",
        "            \"category\": \"Computer Vision\",\n",
        "            \"summary\": \"\"\"Masked Autoencoders (MAE) extend masked language modeling to vision.\n",
        "            Key approach: (1) Randomly mask image patches, (2) Encoder processes visible patches, (3) Decoder reconstructs masked patches.\n",
        "            Advantages: (1) Asymmetric encoder-decoder, (2) Scales to large models, (3) Learns powerful representations.\n",
        "            Results: Self-supervised learning works well for vision similar to NLP.\n",
        "            Impact: Foundation for self-supervised computer vision.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Training data-efficient image transformers & distillation through attention\",\n",
        "            \"authors\": [\"Touvron\", \"Cord\", \"Douze\"],\n",
        "            \"published\": \"2020-12-23\",\n",
        "            \"arxiv_id\": \"2012.12556\",\n",
        "            \"category\": \"Computer Vision\",\n",
        "            \"summary\": \"\"\"DeiT shows how to train vision transformers efficiently on ImageNet-sized datasets.\n",
        "            Key techniques: (1) Knowledge distillation from teacher model, (2) Careful augmentation, (3) Training recipes.\n",
        "            Results: Competitive transformer models without requiring massive datasets like original ViT.\n",
        "            Impact: Made vision transformers practical for standard datasets.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Vision Transformer Slimming\",\n",
        "            \"authors\": [\"Wang\", \"Huang\", \"Song\"],\n",
        "            \"published\": \"2021-06-24\",\n",
        "            \"arxiv_id\": \"2106.02852\",\n",
        "            \"category\": \"Computer Vision\",\n",
        "            \"summary\": \"\"\"Addresses efficiency of vision transformers through architectural optimization.\n",
        "            Key ideas: (1) Reduce patch dimensions, (2) Optimize attention mechanisms, (3) Careful pruning strategies.\n",
        "            Results: Smaller models with competitive accuracy compared to original ViT.\n",
        "            Impact: Made vision transformers more efficient and practical.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Exploring Simple Siamese Representation Learning\",\n",
        "            \"authors\": [\"Chen\", \"He\", \"Fan\"],\n",
        "            \"published\": \"2020-11-04\",\n",
        "            \"arxiv_id\": \"2011.10566\",\n",
        "            \"category\": \"Computer Vision\",\n",
        "            \"summary\": \"\"\"SimSiam shows that contrastive learning works without negative pairs.\n",
        "            Key finding: Simple siamese networks with stop-gradient operation enable self-supervised learning.\n",
        "            Architecture: (1) Two branches with weight sharing, (2) MLP projectors, (3) Stop-gradient operation.\n",
        "            Results: Competitive self-supervised learning without negative sampling or momentum encoder.\n",
        "            Impact: Simplified self-supervised vision learning.\"\"\"\n",
        "        }\n",
        "    ],\n",
        "    \"Multimodal\": [\n",
        "        {\n",
        "            \"title\": \"Learning Transferable Visual Models From Natural Language Supervision\",\n",
        "            \"authors\": [\"Radford\", \"Kim\", \"Hallacy\"],\n",
        "            \"published\": \"2021-02-26\",\n",
        "            \"arxiv_id\": \"2103.00020\",\n",
        "            \"category\": \"Multimodal\",\n",
        "            \"summary\": \"\"\"CLIP learns visual representations from natural language supervision at scale.\n",
        "            Key innovation: Contrastive learning on image-text pairs from internet data.\n",
        "            Architecture: (1) Image encoder (CNN or ViT), (2) Text encoder (transformer), (3) Contrastive loss.\n",
        "            Capability: Zero-shot transfer to new categories described in language.\n",
        "            Impact: Foundation for multimodal models and zero-shot vision.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation\",\n",
        "            \"authors\": [\"Li\", \"Gan\", \"Du\"],\n",
        "            \"published\": \"2022-01-18\",\n",
        "            \"arxiv_id\": \"2201.12086\",\n",
        "            \"category\": \"Multimodal\",\n",
        "            \"summary\": \"\"\"BLIP unifies vision-language understanding and generation in one model.\n",
        "            Key contribution: (1) CapFilt module for filtering noisy captions, (2) Unified encoder-decoder architecture.\n",
        "            Tasks: (1) Image-text retrieval, (2) Visual question answering, (3) Image captioning.\n",
        "            Results: State-of-the-art on VQA, retrieval, and captioning benchmarks.\n",
        "            Impact: Showed unified V-L models can excel at both understanding and generation.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Flamingo: a Visual Language Model for Few-Shot Learning\",\n",
        "            \"authors\": [\"Alayrac\", \"Donahue\", \"Luc\"],\n",
        "            \"published\": \"2022-04-29\",\n",
        "            \"arxiv_id\": \"2204.14198\",\n",
        "            \"category\": \"Multimodal\",\n",
        "            \"summary\": \"\"\"Flamingo enables few-shot learning in vision-language models.\n",
        "            Key design: (1) Interleaved vision and text tokens, (2) Gated cross-attention, (3) Few-shot demonstration capability.\n",
        "            Capability: Learn new tasks from just a few examples.\n",
        "            Results: Strong few-shot performance across diverse vision-language tasks.\n",
        "            Impact: Brought few-shot learning to multimodal domain.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Visual Instruction Tuning\",\n",
        "            \"authors\": [\"Liu\", \"Li\", \"Xu\"],\n",
        "            \"published\": \"2023-04-17\",\n",
        "            \"arxiv_id\": \"2304.08485\",\n",
        "            \"category\": \"Multimodal\",\n",
        "            \"summary\": \"\"\"LLaVA connects vision encoder with large language model via instruction tuning.\n",
        "            Key idea: (1) Freeze pre-trained vision and language models, (2) Learn projection between them, (3) Instruction tune.\n",
        "            Approach: (1) Use CLIP for vision, (2) Use LLaMA for language, (3) Connect with linear projection.\n",
        "            Capability: Follow natural language instructions about images.\n",
        "            Impact: Simplified approach to building multimodal models.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Multimodal Chain-of-Thought Reasoning in Language Models\",\n",
        "            \"authors\": [\"Zhang\", \"Hashimoto\", \"Liang\"],\n",
        "            \"published\": \"2023-02-07\",\n",
        "            \"arxiv_id\": \"2302.00923\",\n",
        "            \"category\": \"Multimodal\",\n",
        "            \"summary\": \"\"\"Extends chain-of-thought reasoning to multimodal settings.\n",
        "            Key idea: (1) Decompose problem into reasoning steps, (2) Show example step-by-step, (3) Improve reasoning across modalities.\n",
        "            Results: Improved performance on complex vision-language tasks requiring reasoning.\n",
        "            Impact: Showed explicit reasoning helps multimodal understanding.\"\"\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(f\"Loaded {sum(len(p) for p in PAPERS_DATABASE.values())} papers\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 4: PROCESS DOCUMENTS\n",
        "# ============================================================================\n",
        "\n",
        "class DocumentProcessor:\n",
        "    \"\"\"Convert papers to document chunks\"\"\"\n",
        "\n",
        "    def __init__(self, chunk_size=800, chunk_overlap=150):\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "        )\n",
        "\n",
        "    def process_papers(self, papers: Dict) -> Tuple[List[str], List[Dict]]:\n",
        "        \"\"\"Process papers into chunks\"\"\"\n",
        "        documents = []\n",
        "        metadata = []\n",
        "\n",
        "        for category, paper_list in papers.items():\n",
        "            for paper in paper_list:\n",
        "                doc_text = f\"\"\"Title: {paper['title']}\n",
        "Authors: {', '.join(paper['authors'])}\n",
        "Category: {category}\n",
        "Published: {paper['published']}\n",
        "\n",
        "Summary:\n",
        "{paper['summary']}\n",
        "\n",
        "ArXiv ID: {paper['arxiv_id']}\n",
        "\"\"\"\n",
        "                chunks = self.text_splitter.split_text(doc_text)\n",
        "\n",
        "                for chunk in chunks:\n",
        "                    documents.append(chunk)\n",
        "                    metadata.append({\n",
        "                        \"title\": paper['title'],\n",
        "                        \"category\": category,\n",
        "                        \"arxiv_id\": paper['arxiv_id'],\n",
        "                        \"authors\": paper['authors'],\n",
        "                    })\n",
        "\n",
        "        return documents, metadata\n",
        "\n",
        "processor = DocumentProcessor()\n",
        "documents, metadata = processor.process_papers(PAPERS_DATABASE)\n",
        "print(f\"Processing documents...\")\n",
        "print(f\"✓ Created {len(documents)} document chunks\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 5: BUILD EMBEDDINGS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"BUILDING RAG SYSTEM\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n1️⃣ Loading embeddings model...\")\n",
        "use_gpu = torch.cuda.is_available()\n",
        "device = \"cuda\" if use_gpu else \"cpu\"\n",
        "print(f\"   Device: {device.upper()}\")\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={\"device\": device}\n",
        ")\n",
        "print(\"✓ Embeddings model ready\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 6: BUILD VECTOR STORE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n2️⃣ Building FAISS vector store...\")\n",
        "\n",
        "vectorstore = FAISS.from_texts(\n",
        "    texts=documents,\n",
        "    embedding=embeddings,\n",
        "    metadatas=metadata\n",
        ")\n",
        "\n",
        "print(f\"✓ Vector store created\")\n",
        "\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 3}\n",
        ")\n",
        "\n",
        "print(\"✓ Retriever configured\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 7: LOAD LLM (FIXED - NO PIPELINE)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n3️⃣ Loading LLM (google/flan-t5-base)...\")\n",
        "\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "if use_gpu:\n",
        "    model = model.to(\"cuda\")\n",
        "\n",
        "print(\"✓ LLM loaded\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 8: SIMPLE LLM WRAPPER (NO PIPELINE)\n",
        "# ============================================================================\n",
        "\n",
        "class SimpleLLM:\n",
        "    \"\"\"Simple LLM wrapper that avoids pipeline issues\"\"\"\n",
        "\n",
        "    def __init__(self, model, tokenizer, device):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "        self.max_length = 512\n",
        "\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        \"\"\"Generate text from prompt\"\"\"\n",
        "        try:\n",
        "            # Tokenize\n",
        "            inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=1024\n",
        "            ).to(self.device)\n",
        "\n",
        "            # Generate\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_length=self.max_length,\n",
        "                num_beams=1,\n",
        "                temperature=0.7,\n",
        "                do_sample=False\n",
        "            )\n",
        "\n",
        "            # Decode\n",
        "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            return f\"Error generating response: {str(e)[:100]}\"\n",
        "\n",
        "llm = SimpleLLM(model, tokenizer, device)\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 9: BUILD RAG CHAIN (MANUAL)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n4️⃣ Creating RAG chain...\")\n",
        "\n",
        "template = \"\"\"You are an expert research assistant. Answer questions about research papers.\n",
        "\n",
        "Papers:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer based on the papers. Be concise and helpful.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "def format_docs(docs):\n",
        "    \"\"\"Format documents\"\"\"\n",
        "    if not docs:\n",
        "        return \"No papers found.\"\n",
        "    formatted = []\n",
        "    for i, doc in enumerate(docs[:3], 1):\n",
        "        title = doc.metadata.get(\"title\", \"Unknown\")\n",
        "        arxiv = doc.metadata.get(\"arxiv_id\", \"N/A\")\n",
        "        formatted.append(f\"[{i}] {title} ({arxiv})\\n{doc.page_content[:300]}...\")\n",
        "    return \"\\n\\n\".join(formatted)\n",
        "\n",
        "def rag_chain_invoke(question: str):\n",
        "    \"\"\"Simple RAG chain\"\"\"\n",
        "    # Retrieve\n",
        "    docs = retriever.invoke(question)\n",
        "    context = format_docs(docs)\n",
        "\n",
        "    # Format prompt\n",
        "    formatted_prompt = template.format(context=context, question=question)\n",
        "\n",
        "    # Generate answer\n",
        "    answer = llm.generate(formatted_prompt)\n",
        "\n",
        "    return answer, docs\n",
        "\n",
        "print(\"✓ RAG chain ready!\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✅ RAG SYSTEM READY!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 10: QUERY FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "def query_research(question: str, verbose=True):\n",
        "    \"\"\"Query the RAG system\"\"\"\n",
        "    if verbose:\n",
        "        print(f\"\\n🔍 Question: {question}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "    try:\n",
        "        # Get answer\n",
        "        answer, docs = rag_chain_invoke(question)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\n💡 Answer:\\n{answer}\\n\")\n",
        "\n",
        "        # Show sources\n",
        "        if verbose and docs:\n",
        "            print(f\"📚 Sources:\")\n",
        "            seen = set()\n",
        "            for doc in docs:\n",
        "                title = doc.metadata.get(\"title\")\n",
        "                if title not in seen:\n",
        "                    seen.add(title)\n",
        "                    arxiv = doc.metadata.get(\"arxiv_id\")\n",
        "                    category = doc.metadata.get(\"category\")\n",
        "                    print(f\"  • {title}\")\n",
        "                    print(f\"    Category: {category} | ArXiv: {arxiv}\")\n",
        "\n",
        "        return answer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 11: TEST QUESTIONS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TESTING WITH QUESTIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "test_questions = [\n",
        "    \"What are transformers and how do they work?\",\n",
        "    \"Explain Vision Transformers and their advantages\",\n",
        "    \"What is CLIP and how does it enable zero-shot learning?\",\n",
        "]\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"QUESTION {i}/{len(test_questions)}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    query_research(question)\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 12: SUMMARY & YOUR QUESTIONS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*80)\n",
        "print(\"✅ COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nSystem Status:\")\n",
        "print(f\"  ✓ Papers: 15 (5 NLP + 5 Vision + 5 Multimodal)\")\n",
        "print(f\"  ✓ Document chunks: {len(documents)}\")\n",
        "print(f\"  ✓ Device: {device.upper()}\")\n",
        "print(f\"  ✓ Status: READY TO USE\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ASK YOUR OWN QUESTIONS!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nUsage:\")\n",
        "print('  query_research(\"Your question here?\")')\n",
        "\n",
        "print(\"\\nExamples:\")\n",
        "print('  query_research(\"What makes BERT different from GPT?\")')\n",
        "print('  query_research(\"How do multimodal models work?\")')\n",
        "print('  query_research(\"Explain self-supervised learning in vision\")')\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 13: YOUR CUSTOM QUESTIONS\n",
        "# ============================================================================\n",
        "\n",
        "# Uncomment and modify to ask your own questions:\n",
        "\n",
        "# query_research(\"What is attention mechanism?\")\n",
        "# query_research(\"How does CLIP work for zero-shot classification?\")\n",
        "# query_research(\"What are the key innovations in Vision Transformers?\")\n",
        "# query_research(\"Explain the difference between supervised and self-supervised learning\")\n",
        "# query_research(\"What papers should I read to understand modern AI?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "56ba4215c9b94192b50fc1e40fa10ef3",
            "037fb92040f44c0791257f1e3f288c30",
            "50ec1d326e704433bac9a59e0f487f13",
            "b1a7c26048d74e9db9b1c44f1f713f8a",
            "f8810c42f1134da0a3efefe47df2e7a1",
            "9ef1aa37e84a4e35b964a944884b0013",
            "90892646c03240f7a3c7e971fbeac837",
            "1c92fca2cf6640adb476672a19e60c7c",
            "747c0da8ef3b43b49e0241d0865e50aa",
            "693969e80fd94535932ca014255a6193",
            "9165150f9bd04246a6594f60ba09d38d",
            "4bbb9433795e4db3b214309f2f1eab07",
            "c2617087a95d40b7bb953688142e4936",
            "570ac49f254e4eee91b56c2a117d6eb0",
            "c518a599a3f84e0f90eb958a851a4c90",
            "02e4d3fe8eb34db683b0be6e8c76beed",
            "0d1727ae68ef49a0a2d12844c745ac2e",
            "c01cb71c13e548eb94521576f2cb1556",
            "8f2d37c579534681b3fbfa8ea6b31638",
            "f914e6701fef420e849a14e8afdb89db",
            "5cd52b69cbb449d5b2c1bc3f8a90ba56",
            "01d020eeda2548c3b1309322b3e97ccf"
          ]
        },
        "id": "2WRBFTZEB_vO",
        "outputId": "814b3454-c6c6-4854-99c0-2a75f076cb0c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ All packages installed!\n",
            "✓ All imports successful!\n",
            "GPU Available: True\n",
            "Loaded 15 papers\n",
            "Processing documents...\n",
            "✓ Created 15 document chunks\n",
            "\n",
            "================================================================================\n",
            "BUILDING RAG SYSTEM\n",
            "================================================================================\n",
            "\n",
            "1️⃣ Loading embeddings model...\n",
            "   Device: CUDA\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56ba4215c9b94192b50fc1e40fa10ef3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Embeddings model ready\n",
            "\n",
            "2️⃣ Building FAISS vector store...\n",
            "✓ Vector store created\n",
            "✓ Retriever configured\n",
            "\n",
            "3️⃣ Loading LLM (google/flan-t5-base)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/282 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4bbb9433795e4db3b214309f2f1eab07"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ LLM loaded\n",
            "\n",
            "4️⃣ Creating RAG chain...\n",
            "✓ RAG chain ready!\n",
            "\n",
            "================================================================================\n",
            "✅ RAG SYSTEM READY!\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "TESTING WITH QUESTIONS\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "QUESTION 1/3\n",
            "================================================================================\n",
            "\n",
            "🔍 Question: What are transformers and how do they work?\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "💡 Answer:\n",
            "What are transformers and how do they work?\n",
            "\n",
            "📚 Sources:\n",
            "  • An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\n",
            "    Category: Computer Vision | ArXiv: 2010.11929\n",
            "  • Attention Is All You Need\n",
            "    Category: NLP | ArXiv: 1706.03762\n",
            "  • Vision Transformer Slimming\n",
            "    Category: Computer Vision | ArXiv: 2106.02852\n",
            "\n",
            "================================================================================\n",
            "QUESTION 2/3\n",
            "================================================================================\n",
            "\n",
            "🔍 Question: Explain Vision Transformers and their advantages\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "💡 Answer:\n",
            "Summary: Vision transformers are a type of image classification system.\n",
            "\n",
            "📚 Sources:\n",
            "  • Vision Transformer Slimming\n",
            "    Category: Computer Vision | ArXiv: 2106.02852\n",
            "  • Training data-efficient image transformers & distillation through attention\n",
            "    Category: Computer Vision | ArXiv: 2012.12556\n",
            "  • An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\n",
            "    Category: Computer Vision | ArXiv: 2010.11929\n",
            "\n",
            "================================================================================\n",
            "QUESTION 3/3\n",
            "================================================================================\n",
            "\n",
            "🔍 Question: What is CLIP and how does it enable zero-shot learning?\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "💡 Answer:\n",
            "Summary: Learning Transferable Visual Models From Natural Language Supervision\n",
            "\n",
            "📚 Sources:\n",
            "  • Flamingo: a Visual Language Model for Few-Shot Learning\n",
            "    Category: Multimodal | ArXiv: 2204.14198\n",
            "  • Learning Transferable Visual Models From Natural Language Supervision\n",
            "    Category: Multimodal | ArXiv: 2103.00020\n",
            "  • Masked Autoencoders Are Scalable Vision Learners\n",
            "    Category: Computer Vision | ArXiv: 2111.06377\n",
            "\n",
            "\n",
            "================================================================================\n",
            "✅ COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "System Status:\n",
            "  ✓ Papers: 15 (5 NLP + 5 Vision + 5 Multimodal)\n",
            "  ✓ Document chunks: 15\n",
            "  ✓ Device: CUDA\n",
            "  ✓ Status: READY TO USE\n",
            "\n",
            "================================================================================\n",
            "ASK YOUR OWN QUESTIONS!\n",
            "================================================================================\n",
            "\n",
            "Usage:\n",
            "  query_research(\"Your question here?\")\n",
            "\n",
            "Examples:\n",
            "  query_research(\"What makes BERT different from GPT?\")\n",
            "  query_research(\"How do multimodal models work?\")\n",
            "  query_research(\"Explain self-supervised learning in vision\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# COMPLETE RAG + EVALUATION SYSTEM\n",
        "# Google Colab - Ready to Use\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# PART A: RAG SYSTEM (From Previous Code)\n",
        "# ============================================================================\n",
        "\n",
        "# CELL 1: Install all packages\n",
        "!pip install -q langchain langchain-community langchain-core\n",
        "!pip install -q faiss-cpu sentence-transformers\n",
        "!pip install -q torch transformers huggingface-hub\n",
        "!pip install -q tqdm rouge-score scikit-learn pandas numpy\n",
        "\n",
        "print(\"✓ All packages installed!\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 2: Imports\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from rouge_score import rouge_scorer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "print(\"✓ All imports successful!\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 3: Paper Database\n",
        "# ============================================================================\n",
        "\n",
        "PAPERS_DATABASE = {\n",
        "    \"NLP\": [\n",
        "        {\n",
        "            \"title\": \"Attention Is All You Need\",\n",
        "            \"authors\": [\"Vaswani\", \"Shazeer\"],\n",
        "            \"published\": \"2017-06-12\",\n",
        "            \"arxiv_id\": \"1706.03762\",\n",
        "            \"category\": \"NLP\",\n",
        "            \"summary\": \"\"\"Transformers use self-attention mechanisms instead of recurrence. Key: multi-head attention, positional encodings, encoder-decoder architecture. Enables parallel processing and fast training.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"BERT: Pre-training of Deep Bidirectional Transformers\",\n",
        "            \"authors\": [\"Devlin\", \"Chang\"],\n",
        "            \"published\": \"2018-10-11\",\n",
        "            \"arxiv_id\": \"1810.04805\",\n",
        "            \"category\": \"NLP\",\n",
        "            \"summary\": \"\"\"BERT introduces bidirectional pre-training with Masked Language Modeling (MLM). Reads context from both directions. Significantly improved GLUE benchmarks.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Language Models are Unsupervised Multitask Learners\",\n",
        "            \"authors\": [\"Radford\", \"Wu\"],\n",
        "            \"published\": \"2019-02-14\",\n",
        "            \"arxiv_id\": \"1902.10165\",\n",
        "            \"category\": \"NLP\",\n",
        "            \"summary\": \"\"\"GPT-2 shows large language models learn multiple tasks without supervision. Models learn translation, summarization, QA naturally from data.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"RoFormer: Enhanced Transformer with Rotary Position Embedding\",\n",
        "            \"authors\": [\"Su\", \"Ahmed\"],\n",
        "            \"published\": \"2021-04-20\",\n",
        "            \"arxiv_id\": \"2104.09864\",\n",
        "            \"category\": \"NLP\",\n",
        "            \"summary\": \"\"\"Rotary Position Embeddings (RoPE) encode positions using rotation matrices. Better extrapolation to longer sequences.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"LLaMA: Open and Efficient Foundation Language Models\",\n",
        "            \"authors\": [\"Touvron\", \"Lavril\"],\n",
        "            \"published\": \"2023-02-27\",\n",
        "            \"arxiv_id\": \"2302.13971\",\n",
        "            \"category\": \"NLP\",\n",
        "            \"summary\": \"\"\"LLaMA-13B outperforms GPT-3 65B on public data. Shows efficient training on public datasets.\"\"\"\n",
        "        }\n",
        "    ],\n",
        "    \"Computer Vision\": [\n",
        "        {\n",
        "            \"title\": \"An Image is Worth 16x16 Words: Transformers for Image Recognition\",\n",
        "            \"authors\": [\"Dosovitskiy\", \"Beyer\"],\n",
        "            \"published\": \"2020-10-22\",\n",
        "            \"arxiv_id\": \"2010.11929\",\n",
        "            \"category\": \"Computer Vision\",\n",
        "            \"summary\": \"\"\"Vision Transformer (ViT) divides images into patches as tokens. Competitive with CNNs on large datasets.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Masked Autoencoders Are Scalable Vision Learners\",\n",
        "            \"authors\": [\"He\", \"Chen\"],\n",
        "            \"published\": \"2021-11-11\",\n",
        "            \"arxiv_id\": \"2111.06377\",\n",
        "            \"category\": \"Computer Vision\",\n",
        "            \"summary\": \"\"\"MAE extends masked language modeling to vision. Randomly mask patches and reconstruct. Self-supervised learning works for vision.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Training data-efficient image transformers\",\n",
        "            \"authors\": [\"Touvron\", \"Cord\"],\n",
        "            \"published\": \"2020-12-23\",\n",
        "            \"arxiv_id\": \"2012.12556\",\n",
        "            \"category\": \"Computer Vision\",\n",
        "            \"summary\": \"\"\"DeiT shows vision transformers work on standard ImageNet. Uses knowledge distillation and careful training.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Vision Transformer Slimming\",\n",
        "            \"authors\": [\"Wang\", \"Huang\"],\n",
        "            \"published\": \"2021-06-24\",\n",
        "            \"arxiv_id\": \"2106.02852\",\n",
        "            \"category\": \"Computer Vision\",\n",
        "            \"summary\": \"\"\"Reduces ViT complexity while maintaining performance. Architectural optimizations and pruning.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Exploring Simple Siamese Representation Learning\",\n",
        "            \"authors\": [\"Chen\", \"He\"],\n",
        "            \"published\": \"2020-11-04\",\n",
        "            \"arxiv_id\": \"2011.10566\",\n",
        "            \"category\": \"Computer Vision\",\n",
        "            \"summary\": \"\"\"SimSiam shows contrastive learning works without negative pairs. Simplified self-supervised learning.\"\"\"\n",
        "        }\n",
        "    ],\n",
        "    \"Multimodal\": [\n",
        "        {\n",
        "            \"title\": \"Learning Transferable Visual Models From Natural Language Supervision\",\n",
        "            \"authors\": [\"Radford\", \"Kim\"],\n",
        "            \"published\": \"2021-02-26\",\n",
        "            \"arxiv_id\": \"2103.00020\",\n",
        "            \"category\": \"Multimodal\",\n",
        "            \"summary\": \"\"\"CLIP learns from image-text pairs. Enables zero-shot classification by understanding image-text relationships.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"BLIP: Bootstrapping Language-Image Pre-training\",\n",
        "            \"authors\": [\"Li\", \"Gan\"],\n",
        "            \"published\": \"2022-01-18\",\n",
        "            \"arxiv_id\": \"2201.12086\",\n",
        "            \"category\": \"Multimodal\",\n",
        "            \"summary\": \"\"\"BLIP unifies understanding and generation. Bootstraps on noisy web data.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Flamingo: a Visual Language Model for Few-Shot Learning\",\n",
        "            \"authors\": [\"Alayrac\", \"Donahue\"],\n",
        "            \"published\": \"2022-04-29\",\n",
        "            \"arxiv_id\": \"2204.14198\",\n",
        "            \"category\": \"Multimodal\",\n",
        "            \"summary\": \"\"\"Flamingo enables few-shot learning in multimodal models. Learns from few examples.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Visual Instruction Tuning\",\n",
        "            \"authors\": [\"Liu\", \"Li\"],\n",
        "            \"published\": \"2023-04-17\",\n",
        "            \"arxiv_id\": \"2304.08485\",\n",
        "            \"category\": \"Multimodal\",\n",
        "            \"summary\": \"\"\"LLaVA connects vision and language models. Follows natural language instructions about images.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"Multimodal Chain-of-Thought Reasoning\",\n",
        "            \"authors\": [\"Zhang\", \"Hashimoto\"],\n",
        "            \"published\": \"2023-02-07\",\n",
        "            \"arxiv_id\": \"2302.00923\",\n",
        "            \"category\": \"Multimodal\",\n",
        "            \"summary\": \"\"\"Extends chain-of-thought to multimodal. Improves reasoning on complex vision-language tasks.\"\"\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(f\"Loaded {sum(len(p) for p in PAPERS_DATABASE.values())} papers\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 4: Process Documents\n",
        "# ============================================================================\n",
        "\n",
        "class DocumentProcessor:\n",
        "    def __init__(self, chunk_size=800, chunk_overlap=150):\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "        )\n",
        "\n",
        "    def process_papers(self, papers: Dict) -> Tuple[List[str], List[Dict]]:\n",
        "        documents = []\n",
        "        metadata = []\n",
        "\n",
        "        for category, paper_list in papers.items():\n",
        "            for paper in paper_list:\n",
        "                doc_text = f\"\"\"Title: {paper['title']}\n",
        "Authors: {', '.join(paper['authors'])}\n",
        "Category: {category}\n",
        "Published: {paper['published']}\n",
        "\n",
        "{paper['summary']}\n",
        "\n",
        "ArXiv ID: {paper['arxiv_id']}\"\"\"\n",
        "\n",
        "                chunks = self.text_splitter.split_text(doc_text)\n",
        "                for chunk in chunks:\n",
        "                    documents.append(chunk)\n",
        "                    metadata.append({\n",
        "                        \"title\": paper['title'],\n",
        "                        \"category\": category,\n",
        "                        \"arxiv_id\": paper['arxiv_id'],\n",
        "                    })\n",
        "\n",
        "        return documents, metadata\n",
        "\n",
        "processor = DocumentProcessor()\n",
        "documents, metadata = processor.process_papers(PAPERS_DATABASE)\n",
        "print(f\"✓ Created {len(documents)} document chunks\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 5: Build RAG System\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"BUILDING RAG SYSTEM\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n1️⃣ Loading embeddings...\")\n",
        "use_gpu = torch.cuda.is_available()\n",
        "device = \"cuda\" if use_gpu else \"cpu\"\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={\"device\": device}\n",
        ")\n",
        "print(f\"✓ Embeddings ready (device: {device})\")\n",
        "\n",
        "print(\"\\n2️⃣ Building FAISS vector store...\")\n",
        "vectorstore = FAISS.from_texts(texts=documents, embedding=embeddings, metadatas=metadata)\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
        "print(f\"✓ Vector store created ({len(documents)} chunks)\")\n",
        "\n",
        "print(\"\\n3️⃣ Loading LLM...\")\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "if use_gpu:\n",
        "    model = model.to(\"cuda\")\n",
        "print(\"✓ LLM loaded\")\n",
        "\n",
        "class SimpleLLM:\n",
        "    def __init__(self, model, tokenizer, device):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        try:\n",
        "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(self.device)\n",
        "            outputs = self.model.generate(**inputs, max_length=512, num_beams=1, temperature=0.7, do_sample=False)\n",
        "            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        except:\n",
        "            return \"Error generating response\"\n",
        "\n",
        "llm = SimpleLLM(model, tokenizer, device)\n",
        "\n",
        "print(\"\\n4️⃣ Creating RAG chain...\")\n",
        "template = \"\"\"You are an expert research assistant. Answer questions about papers.\n",
        "\n",
        "Papers:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer based on papers. Be concise.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
        "\n",
        "def format_docs(docs):\n",
        "    if not docs:\n",
        "        return \"No papers found.\"\n",
        "    formatted = []\n",
        "    for i, doc in enumerate(docs[:3], 1):\n",
        "        title = doc.metadata.get(\"title\", \"Unknown\")\n",
        "        arxiv = doc.metadata.get(\"arxiv_id\", \"N/A\")\n",
        "        formatted.append(f\"[{i}] {title} ({arxiv})\\n{doc.page_content[:250]}...\")\n",
        "    return \"\\n\\n\".join(formatted)\n",
        "\n",
        "def rag_chain(question: str):\n",
        "    docs = retriever.invoke(question)\n",
        "    context = format_docs(docs)\n",
        "    formatted_prompt = template.format(context=context, question=question)\n",
        "    answer = llm.generate(formatted_prompt)\n",
        "    return answer, docs\n",
        "\n",
        "print(\"✓ RAG chain ready!\")\n",
        "\n",
        "# ============================================================================\n",
        "# PART B: EVALUATION SYSTEM\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVALUATION METRICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Simple text utilities (no NLTK!)\n",
        "def simple_tokenize(text: str) -> List[str]:\n",
        "    import re\n",
        "    text = text.lower()\n",
        "    return re.findall(r'\\w+', text)\n",
        "\n",
        "# Evaluation metrics\n",
        "class EvalMetrics:\n",
        "    @staticmethod\n",
        "    def rouge(ref: str, gen: str) -> Dict[str, float]:\n",
        "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "        scores = scorer.score(ref, gen)\n",
        "        return {\n",
        "            'rouge1': scores['rouge1'].fmeasure,\n",
        "            'rouge2': scores['rouge2'].fmeasure,\n",
        "            'rougeL': scores['rougeL'].fmeasure,\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def semantic_sim(text1: str, text2: str) -> float:\n",
        "        try:\n",
        "            emb1 = np.array(embeddings.embed_query(text1)).reshape(1, -1)\n",
        "            emb2 = np.array(embeddings.embed_query(text2)).reshape(1, -1)\n",
        "            return float(cosine_similarity(emb1, emb2)[0][0])\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    @staticmethod\n",
        "    def jaccard(text1: str, text2: str) -> float:\n",
        "        set1 = set(simple_tokenize(text1))\n",
        "        set2 = set(simple_tokenize(text2))\n",
        "        if not set1 and not set2:\n",
        "            return 1.0\n",
        "        intersection = len(set1 & set2)\n",
        "        union = len(set1 | set2)\n",
        "        return intersection / union if union > 0 else 0.0\n",
        "\n",
        "    @staticmethod\n",
        "    def mrr(retrieved: List[str], relevant: List[str]) -> float:\n",
        "        for i, doc_id in enumerate(retrieved):\n",
        "            if doc_id in relevant:\n",
        "                return 1.0 / (i + 1)\n",
        "        return 0.0\n",
        "\n",
        "    @staticmethod\n",
        "    def precision_at_k(retrieved: List[str], relevant: List[str], k: int = 3) -> float:\n",
        "        if k == 0:\n",
        "            return 0.0\n",
        "        top_k = retrieved[:k]\n",
        "        correct = sum(1 for doc in top_k if doc in relevant)\n",
        "        return correct / k\n",
        "\n",
        "print(\"✓ Evaluation metrics ready\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 6: Evaluation Dataset\n",
        "# ============================================================================\n",
        "\n",
        "EVAL_DATA = [\n",
        "    {\n",
        "        \"question\": \"What are transformers?\",\n",
        "        \"expected\": \"Transformers use self-attention mechanisms for parallel processing.\",\n",
        "        \"papers\": [\"1706.03762\"]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Explain Vision Transformers\",\n",
        "        \"expected\": \"ViT divides images into patches and applies transformer architecture.\",\n",
        "        \"papers\": [\"2010.11929\"]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is CLIP?\",\n",
        "        \"expected\": \"CLIP learns from image-text pairs enabling zero-shot classification.\",\n",
        "        \"papers\": [\"2103.00020\"]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is BERT?\",\n",
        "        \"expected\": \"BERT uses bidirectional pre-training with masked language modeling.\",\n",
        "        \"papers\": [\"1810.04805\"]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How do multimodal models work?\",\n",
        "        \"expected\": \"They combine vision and language through connection layers.\",\n",
        "        \"papers\": [\"2201.12086\", \"2304.08485\"]\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"Loaded {len(EVAL_DATA)} evaluation questions\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 7: Run Evaluation\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"RUNNING EVALUATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "results = []\n",
        "for i, test in enumerate(EVAL_DATA, 1):\n",
        "    print(f\"\\n{i}. {test['question']}\")\n",
        "\n",
        "    start = time.time()\n",
        "    answer, docs = rag_chain(test['question'])\n",
        "    latency = time.time() - start\n",
        "\n",
        "    retrieved_papers = [doc.metadata.get('arxiv_id') for doc in docs]\n",
        "\n",
        "    # Calculate metrics\n",
        "    rouge = EvalMetrics.rouge(test['expected'], answer)\n",
        "    semantic = EvalMetrics.semantic_sim(test['expected'], answer)\n",
        "    jaccard = EvalMetrics.jaccard(test['expected'], answer)\n",
        "    mrr = EvalMetrics.mrr(retrieved_papers, test['papers'])\n",
        "    precision = EvalMetrics.precision_at_k(retrieved_papers, test['papers'])\n",
        "\n",
        "    result = {\n",
        "        \"question\": test['question'],\n",
        "        \"latency\": latency,\n",
        "        \"rouge1\": rouge['rouge1'],\n",
        "        \"rouge2\": rouge['rouge2'],\n",
        "        \"rougeL\": rouge['rougeL'],\n",
        "        \"semantic\": semantic,\n",
        "        \"jaccard\": jaccard,\n",
        "        \"mrr\": mrr,\n",
        "        \"precision_at_3\": precision,\n",
        "    }\n",
        "    results.append(result)\n",
        "\n",
        "    print(f\"   Latency: {latency:.2f}s\")\n",
        "    print(f\"   ROUGE-1: {rouge['rouge1']:.3f}\")\n",
        "    print(f\"   Semantic: {semantic:.3f}\")\n",
        "    print(f\"   MRR: {mrr:.3f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 8: Results Summary\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "print(\"\\nDETAILED RESULTS:\")\n",
        "print(df[['question', 'latency', 'rouge1', 'semantic', 'mrr']].to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"AGGREGATE METRICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "agg = {\n",
        "    \"Avg Latency (sec)\": df['latency'].mean(),\n",
        "    \"ROUGE-1\": df['rouge1'].mean(),\n",
        "    \"ROUGE-2\": df['rouge2'].mean(),\n",
        "    \"ROUGE-L\": df['rougeL'].mean(),\n",
        "    \"Semantic Similarity\": df['semantic'].mean(),\n",
        "    \"Jaccard Similarity\": df['jaccard'].mean(),\n",
        "    \"MRR\": df['mrr'].mean(),\n",
        "    \"Precision@3\": df['precision_at_3'].mean(),\n",
        "}\n",
        "\n",
        "for name, value in agg.items():\n",
        "    bar = \"█\" * int(value * 20) + \"░\" * (20 - int(value * 20))\n",
        "    print(f\"{name:.<35} {bar} {value:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 9: Interpretation\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"INTERPRETATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def status(score):\n",
        "    if score >= 0.7:\n",
        "        return \"🟢 Excellent\"\n",
        "    elif score >= 0.5:\n",
        "        return \"🟡 Good\"\n",
        "    elif score >= 0.3:\n",
        "        return \"🟠 Fair\"\n",
        "    else:\n",
        "        return \"🔴 Poor\"\n",
        "\n",
        "print(f\"\\nRUGE-1: {agg['ROUGE-1']:.4f} {status(agg['ROUGE-1'])}\")\n",
        "print(f\"Semantic Similarity: {agg['Semantic Similarity']:.4f} {status(agg['Semantic Similarity'])}\")\n",
        "print(f\"MRR: {agg['MRR']:.4f} {status(agg['MRR'])}\")\n",
        "print(f\"Precision@3: {agg['Precision@3']:.4f} {status(agg['Precision@3'])}\")\n",
        "\n",
        "latency = agg['Avg Latency (sec)']\n",
        "if latency < 5:\n",
        "    latency_status = \"🟢 Excellent\"\n",
        "elif latency < 10:\n",
        "    latency_status = \"🟡 Good\"\n",
        "else:\n",
        "    latency_status = \"🟠 Fair\"\n",
        "print(f\"Latency: {latency:.2f}s {latency_status}\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 10: Done!\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✅ COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n✓ RAG System: READY\")\n",
        "print(f\"✓ Evaluation: COMPLETE\")\n",
        "print(f\"✓ Results: {len(results)} questions evaluated\")\n",
        "print(f\"\\nTo ask more questions, run:\")\n",
        "print('  answer, docs = rag_chain(\"Your question here?\")')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7033d328456c4be6a201d52c4870f4c1",
            "7b36604aafa648f59ddc4891d08dab0b",
            "486dbfc600eb43199eaa08cf992f4bb4",
            "85fdd57f64154e7894cdc4e93d25c17d",
            "e0444194af25486db071313d2f71c85b",
            "d31e6c9be06640a49f22882abd8d81b0",
            "425a84dd1b704fa891dd252b6b0c31df",
            "98e65565a3e2497499adeb253bdd6e90",
            "7ba0ffa369264038a6a1bcecaf8f9d46",
            "8324313d6bbc4be1996bf4ef10b2b841",
            "983ca30c4c724bc7a6edff62961e9377",
            "1289d310a0c74dacae8664ba5c898054",
            "9a4c48ae10514810a0201c8003fd0045",
            "0ca00c6bfb5444c0af909bcd1984c0ae",
            "265823666e5f492d8f64e9f27f1f4c54",
            "d56402835ec24b7e954160b115c419b9",
            "0d8cdc488a2044ddb8990a79f9c7da8b",
            "3def824c40674786b98bb334bc5df144",
            "1bc5d76625ea4a6797e96569f334c4b0",
            "9d4e507262d248559697c99150025ba0",
            "84d52f372e34401fa8018d4e444efab8",
            "c0a659d3ea7747b0b305fcfed120cd59"
          ]
        },
        "id": "JMWs9JwkE8bY",
        "outputId": "e23595a6-f9ce-4624-9861-42245e9e178d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ All packages installed!\n",
            "✓ All imports successful!\n",
            "Loaded 15 papers\n",
            "✓ Created 15 document chunks\n",
            "\n",
            "================================================================================\n",
            "BUILDING RAG SYSTEM\n",
            "================================================================================\n",
            "\n",
            "1️⃣ Loading embeddings...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7033d328456c4be6a201d52c4870f4c1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Embeddings ready (device: cuda)\n",
            "\n",
            "2️⃣ Building FAISS vector store...\n",
            "✓ Vector store created (15 chunks)\n",
            "\n",
            "3️⃣ Loading LLM...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/282 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1289d310a0c74dacae8664ba5c898054"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ LLM loaded\n",
            "\n",
            "4️⃣ Creating RAG chain...\n",
            "✓ RAG chain ready!\n",
            "\n",
            "================================================================================\n",
            "EVALUATION METRICS\n",
            "================================================================================\n",
            "✓ Evaluation metrics ready\n",
            "\n",
            "Loaded 5 evaluation questions\n",
            "\n",
            "================================================================================\n",
            "RUNNING EVALUATION\n",
            "================================================================================\n",
            "\n",
            "1. What are transformers?\n",
            "   Latency: 0.16s\n",
            "   ROUGE-1: 0.545\n",
            "   Semantic: 0.545\n",
            "   MRR: 1.000\n",
            "\n",
            "2. Explain Vision Transformers\n",
            "   Latency: 10.60s\n",
            "   ROUGE-1: 0.010\n",
            "   Semantic: 0.294\n",
            "   MRR: 0.333\n",
            "\n",
            "3. What is CLIP?\n",
            "   Latency: 0.27s\n",
            "   ROUGE-1: 0.222\n",
            "   Semantic: 0.418\n",
            "   MRR: 1.000\n",
            "\n",
            "4. What is BERT?\n",
            "   Latency: 0.23s\n",
            "   ROUGE-1: 0.400\n",
            "   Semantic: 0.494\n",
            "   MRR: 1.000\n",
            "\n",
            "5. How do multimodal models work?\n",
            "   Latency: 0.23s\n",
            "   ROUGE-1: 0.000\n",
            "   Semantic: 0.371\n",
            "   MRR: 0.500\n",
            "\n",
            "================================================================================\n",
            "EVALUATION RESULTS\n",
            "================================================================================\n",
            "\n",
            "DETAILED RESULTS:\n",
            "                      question   latency   rouge1  semantic      mrr\n",
            "        What are transformers?  0.157882 0.545455  0.544881 1.000000\n",
            "   Explain Vision Transformers 10.603743 0.010309  0.293682 0.333333\n",
            "                 What is CLIP?  0.273719 0.222222  0.417820 1.000000\n",
            "                 What is BERT?  0.229216 0.400000  0.493835 1.000000\n",
            "How do multimodal models work?  0.234507 0.000000  0.371090 0.500000\n",
            "\n",
            "================================================================================\n",
            "AGGREGATE METRICS\n",
            "================================================================================\n",
            "Avg Latency (sec).................. █████████████████████████████████████████████ 2.2998\n",
            "ROUGE-1............................ ████░░░░░░░░░░░░░░░░ 0.2356\n",
            "ROUGE-2............................ ██░░░░░░░░░░░░░░░░░░ 0.1197\n",
            "ROUGE-L............................ ████░░░░░░░░░░░░░░░░ 0.2089\n",
            "Semantic Similarity................ ████████░░░░░░░░░░░░ 0.4243\n",
            "Jaccard Similarity................. ██░░░░░░░░░░░░░░░░░░ 0.1434\n",
            "MRR................................ ███████████████░░░░░ 0.7667\n",
            "Precision@3........................ ██████░░░░░░░░░░░░░░ 0.3333\n",
            "\n",
            "================================================================================\n",
            "INTERPRETATION\n",
            "================================================================================\n",
            "\n",
            "RUGE-1: 0.2356 🔴 Poor\n",
            "Semantic Similarity: 0.4243 🟠 Fair\n",
            "MRR: 0.7667 🟢 Excellent\n",
            "Precision@3: 0.3333 🟠 Fair\n",
            "Latency: 2.30s 🟢 Excellent\n",
            "\n",
            "================================================================================\n",
            "✅ COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "✓ RAG System: READY\n",
            "✓ Evaluation: COMPLETE\n",
            "✓ Results: 5 questions evaluated\n",
            "\n",
            "To ask more questions, run:\n",
            "  answer, docs = rag_chain(\"Your question here?\")\n"
          ]
        }
      ]
    }
  ]
}